79,79,Jonathan W Pillow,UT Austin,Jonathan W Pillow,UT Austin,Weston-Watkins Hinge Loss and Ordered Partitions.
80,80,James Bergstra,Kindred,Yoshua Bengio,University of Montreal,On the Equivalence between Online and Private Learnability beyond Binary Classification.
81,81,Youngmin Cho,"University of California, San Diego",Lawrence Saul,UC San Diego,Breaking the Sample Size Barrier in Model-Based Reinforcement Learning with a Generative Model.
82,82,Barry W Chai,Stanford,Fei-Fei Li,Princeton University,A Finite-Time Analysis of Two Time-Scale Actor-Critic Methods.
83,83,Yoshinobu Kawahara,Osaka University / RIKEN,Jeff A Bilmes,"University of Washington, Seattle",Reinforcement Learning for Control with Multiple Frequencies.
84,84,Katherine Heller,Duke,Nick Chater,None,Bridging Imagination and Reality for Model-Based Deep Reinforcement Learning.
85,85,Kwang In Kim,Saarland University,Matthias Hein,University of TÃ¼bingen,First Order Constrained Optimization in Policy Space.
86,86,Jonathan Chang,Facebook,David Blei,Columbia University,DisCor: Corrective Feedback in Reinforcement Learning via Distribution Correction.
87,87,Chong Wang,ByteDance Inc.,David Blei,Columbia University,Natural Policy Gradient Primal-Dual Method for Constrained Markov Decision Processes.
88,88,Hamed Pirsiavash,University of California Irvine,Charless Fowlkes,UC Irvine,MOPO: Model-based Offline Policy Optimization.
89,89,Jonathan Huang,google.com,Carlos Guestrin,University of Washington,Trust the Model When It Is Confident: Masked Model-based Actor-Critic.
90,90,Le Song,Carnegie Mellon University,Eric Xing,Petuum Inc. /  Carnegie Mellon University,Pipeline PSRO: A Scalable Approach for Finding Approximate Nash Equilibria in Large Games.
91,91,Raman Arora,Johns Hopkins University,Raman Arora,Johns Hopkins University,Model-Based Multi-Agent RL in Zero-Sum Markov Games with Near-Optimal Sample Complexity.
92,92,Matthew B Blaschko,KU Leuven,Andreas Bartels,Centre for Integrative Neuroscience,MOReL: Model-Based Offline Reinforcement Learning.
93,93,Wolf Vanpaemel,KULeuven,Wolf Vanpaemel,KULeuven,Variance-Reduced Off-Policy TDC Learning: Non-Asymptotic Convergence Analysis.
94,94,Hamed Valizadegan,Michigan State University,Jianchang Mao,Yahoo! Labs,Independent Policy Gradient Methods for Competitive Reinforcement Learning.
95,95,Hamid R Maei,Stanford University,Rich Sutton,"DeepMind, U Alberta",Provably Good Batch Reinforcement Learning Without Great Exploration.
96,96,Vivek Farias,Massachusetts Institute of Technology,Devavrat Shah,Massachusetts Institute of Technology,(De)Randomized Smoothing for Certifiable Defense against Patch Attacks.
97,97,Bing Bai,NEC Labs America,Mehryar Mohri,Google Research & Courant Institute of Mathematical Sciences,Contrastive Learning with Adversarial Examples .
98,98,Vijay Desai,None,Ciamac C Moallemi,Columbia University,CoADNet: Collaborative Aggregation-and-Distribution Networks for Co-Salient Object Detection.
