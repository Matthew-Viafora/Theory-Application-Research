1619,1619,Baback Moghaddam,Caltech,Kevin P Murphy,Google,Tackling the Objective Inconsistency Problem in Heterogeneous Federated Optimization.
1620,1620,Matthew Wilder,University of Colorado at Boulder,Mike Mozer,Google Research and U. Colorado Boulder,Conic Descent and its Application to Memory-efficient Optimization over Positive Semidefinite Matrices.
1621,1621,Alexander Ihler,UC Irvine,Padhraic Smyth,"University of California, Irvine",A mean-field analysis of two-player zero-sum games.
1622,1622,Hamed Pirsiavash,University of California Irvine,Charless Fowlkes,UC Irvine,Robust Federated Learning: The Case of Affine Distribution Shifts.
1623,1623,Joao V Graca,L2F INESC-ID Lisboa,Fernando Pereira,Google,Learning compositional functions via multiplicative weight updates.
1624,1624,Jean-Pascal Pfister,Cambridge University,Mate Lengyel,University of Cambridge,Stochastic Optimization for Performative Prediction.
1625,1625,Aaron Courville,U. Montreal,Aaron Courville,U. Montreal,Conformal Symplectic and Relativistic Optimization.
1626,1626,Antonio Torralba,Massachusetts Institute of Technology,Antonio Torralba,Massachusetts Institute of Technology,On Power Laws in Deep Ensembles.
1627,1627,Vivek Farias,Massachusetts Institute of Technology,Devavrat Shah,Massachusetts Institute of Technology,Residual Distillation: Towards Portable Deep Neural Networks without Shortcuts.
1628,1628,Peter Orbanz,Columbia University,Peter Orbanz,Columbia University,Bayesian Deep Learning and a Probabilistic Perspective of Generalization.
1629,1629,Jake Bouvrie,Massachusetts Institute of Technology,Tomaso Poggio,MIT,Self-Distillation as Instance-Specific Label Smoothing.
1630,1630,Tomer D Ullman,Massachusetts Institute of Technology,Josh Tenenbaum,MIT,On the training dynamics of deep networks with $L_2$ regularization.
1631,1631,Yee Whye Teh,"University of Oxford, DeepMind",DILAN Gorur,DeepMind,Reconciling Modern Deep Learning with Traditional Optimization Analyses: The Intrinsic Learning Rate.
1632,1632,Theodore Perkins,Ottawa Hospital Research Institute,Theodore Perkins,Ottawa Hospital Research Institute,Batch Normalization Biases Residual Blocks Towards the Identity Function in Deep Networks.
1633,1633,Erik Sudderth,Brown University,Erik Sudderth,Brown University,Numerically Solving Parametric Families of High-Dimensional Kolmogorov Partial Differential Equations via Deep Learning.
1634,1634,Erik Sudderth,Brown University,Erik Sudderth,Brown University,Bad Global Minima Exist and SGD Can Reach Them.
1635,1635,Vijay Desai,None,Ciamac C Moallemi,Columbia University,The Surprising Simplicity of the Early-Time Learning Dynamics of Neural Networks.
1636,1636,Vinod Nair,University of Toronto,Geoffrey E Hinton,Google & University of Toronto,Ensemble Distillation for Robust Model Fusion in Federated Learning.
1637,1637,Ian H Stevenson,University of Connecticut,Konrad Koerding,None,On Warm-Starting Neural Network Training.
1638,1638,Guillermo Cecchi,IBM Research,Jean-Baptiste Poline,CEA-INSERM,Predicting Training Time Without Training .
