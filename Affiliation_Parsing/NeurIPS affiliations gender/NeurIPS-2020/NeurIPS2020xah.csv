,Unnamed: 0,first-author,first-author-affiliation,last-author,last-author-affiliation,title
139,139,Charles Kemp,Carnegie Mellon University,Alan Jern,Rose-Hulman Institute of Technology,ShiftAddNet: A Hardware-Inspired Deep Network.
140,140,Yiming Ying,State University of New York at Albany,Mark A Girolami,University of Glasgow,Pruning neural networks without any data by iteratively conserving synaptic flow.
141,141,Lawrence Cayton,Max Planck Institute for Biological Cybernetics,Lawrence Cayton,Max Planck Institute for Biological Cybernetics,Attribution Preservation in Network Compression for Reliable Network Interpretation.
142,142,Zenglin Xu,University of Electronic Science & Technology of China,Zhirong Yang,Aalto University,Language as a Cognitive Tool to Imagine Goals in Curiosity Driven Exploration.
143,143,Bharath Sriperumbudur,The Pennsylvania State University,Bernhard Schölkopf,"MPI for Intelligent Systems, Tübingen",An Efficient Asynchronous Method for Integrating Evolutionary and Gradient-based Policy Search.
144,144,Kate Saenko,UMass Lowell,Trevor Darrell,UC Berkeley,Model-based Reinforcement Learning for Semi-Markov Decision Processes with Neural ODEs.
145,145,Chun-Nan Hsu,USC,Yuh-Jye Lee,National Taiwan University of Science and Technology,Refactoring Policy for Compositional Generalizability using Self-Supervised Object Proposals.
146,146,Wu-Jun Li,Nanjing University,Zhihua Zhang,Shanghai Jiao Tong University,Cooperative Heterogeneous Deep Reinforcement Learning.
147,147,Arthur Gretton,"Gatsby Unit, UCL",Bharath Sriperumbudur,The Pennsylvania State University,The Mean-Squared Error of Double Q-Learning.
148,148,Peter Orbanz,Columbia University,Peter Orbanz,Columbia University,Novelty Search in Representational Space for Sample Efficient Exploration.
149,149,HongJing Lu,UCLA,Alan L Yuille,UCLA,Learning Implicit Credit Assignment for Cooperative Multi-Agent Reinforcement Learning.
150,150,Baback Moghaddam,Caltech,Kevin P Murphy,Google,Decentralized TD Tracking with Linear Function Approximation and its Finite-Time Analysis.
151,151,Peter Sollich,King's College London,Camille Coti,INRIA Saclay-Île de France,Robust Multi-Agent Reinforcement Learning with Model Uncertainty.
152,152,Jake Bouvrie,Massachusetts Institute of Technology,Tomaso Poggio,MIT,Bayesian Multi-type Mean Field Multi-agent Imitation Learning.
153,153,Piyush Rai,Duke University,Hal Daumé III,Univ of Maryland / Microsoft Research,Emergent Complexity and Zero-shot Transfer via Unsupervised Environment Design.
154,154,Samuel Rota Bulò,Università Ca' Foscari,Marcello Pelillo,Università Ca' Foscari di Venezia,Inverse Rational Control with Partially Observable Continuous Nonlinear Dynamics.
155,155,Samuel J Gershman,Harvard University,Josh Tenenbaum,MIT,Safe Reinforcement Learning via Curriculum Induction.
156,156,M. Pawan Kumar,Stanford University,Daphne Koller,insitro,On the Stability and Convergence of Robust Adversarial Reinforcement Learning: A Case Study on Linear Quadratic Systems.
157,157,Piyush Rai,Duke University,Hal Daumé III,Univ of Maryland / Microsoft Research,Sample Complexity of Asynchronous Q-Learning: Sharper Analysis and Variance Reduction.
158,158,Arkadas Ozakin,Georgia Institute of Technology,Alexander Gray,Skytree Inc. and Georgia Tech,Off-Policy Evaluation via the Regularized Lagrangian.
