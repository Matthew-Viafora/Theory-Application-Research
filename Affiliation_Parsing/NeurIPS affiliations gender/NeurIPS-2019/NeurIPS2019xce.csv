1119,1119,Yaqi Xie,National University of Singapore,Harold Soh,National University of Singapore (NUS),Embedding Symbolic Knowledge into Deep Networks.
1120,1120,Yu Meng,University of Illinois at Urbana-Champaign,Jiawei Han,UIUC,Spherical Text Embedding.
1121,1121,Liwei Wu,"University of California, Davis",James Sharpnack,UC Davis,Stochastic Shared Embeddings: Data-driven Regularization of Embedding Layers.
1122,1122,Cyprien de Masson d'Autume,Google DeepMind,Dani Yogatama,DeepMind,Episodic Memory in Lifelong Language Learning.
1123,1123,Guillaume Lample,Facebook AI Research,Herve Jegou,Facebook AI Research,Large Memory Layers with Product Keys.
1124,1124,Spencer Frei,UCLA,Quanquan Gu,UCLA,Algorithm-Dependent Generalization Bounds for Overparameterized Deep Residual Networks.
1125,1125,Chulhee Yun,MIT,Ali Jadbabaie,MIT,Are deep ResNets provably better than linear predictors?.
1126,1126,Ravi Kumar,Google,Joshua Wang,Google Research,Efficient Rematerialization for Deep Networks.
1127,1127,Guodong Zhang,University of Toronto,Roger Grosse,University of Toronto,Fast Convergence of Natural Gradient Descent for Over-Parameterized Neural Networks.
1128,1128,Devansh Arpit,Salesforce/MILA,Yoshua Bengio,Mila - University of Montreal,How to Initialize your Network? Robust Initialization for WeightNorm & ResNets.
1129,1129,Lei Wu,Princeton University,Chao Ma,Princeton University,  Global Convergence of Gradient Descent  for Deep Linear Residual Networks.
1130,1130,Tristan Milne,University of Toronto,Tristan Milne,University of Toronto,Piecewise Strong Convexity of Neural Networks.
1131,1131,Thijs Vogels,EPFL,Martin Jaggi,EPFL,PowerSGD: Practical Low-Rank Gradient Compression for Distributed Optimization.
1132,1132,Yatin Nandwani,Indian Institute Of Technology Delhi,Parag Singla,Indian Institute of Technology Delhi,A Primal Dual Formulation For Deep Learning With Constraints.
1133,1133,Ganlin Song,Yale University,John Lafferty,Yale University,Surfing: Iterative Optimization Over Incrementally Trained Deep Networks.
1134,1134,Igor Colin,Huawei,Kevin Scaman,Huawei Noah's Ark Lab,Theoretical Limits of Pipeline Parallel Optimization and Application to Distributed Deep Learning.
1135,1135,Qing Qu,New York University,Zhihui Zhu,Johns Hopkins University,A Nonconvex Approach for Exact and Efficient Multichannel Sparse Blind Deconvolution.
1136,1136,Huong Ha,Deakin University,Svetha Venkatesh,Deakin University,Bayesian Optimization with Unknown Search Space.
1137,1137,Viet Anh Nguyen,Stanford University,Wolfram Wiesemann,Imperial College,Calculating Optimistic Likelihoods Using (Geodesically) Convex Optimization.
1138,1138,Kiran Thekumparampil,Univ. of Illinois at Urbana-Champaign,Sewoong Oh,University of Washington,Efficient Algorithms for Smooth Minimax Optimization.
