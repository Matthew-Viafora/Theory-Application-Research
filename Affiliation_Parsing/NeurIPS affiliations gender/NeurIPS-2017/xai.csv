159,159,Yatao Bian,ETH Zürich,Joachim M Buhmann,ETH Zurich,Non-monotone Continuous DR-submodular  Maximization: Structure and Algorithms.
160,160,Gang Wang,University of Minnesota,Jie Chen,Beijing Institute of Technology,Solving Most Systems of Random Quadratic Equations.
161,161,Mahdi Soltanolkotabi,University of Southern california,Mahdi Soltanolkotabi,University of Southern california,Learning ReLUs via Gradient Descent.
162,162,Zhengyuan Zhou,Stanford University,Peter W Glynn,Stanford University,Stochastic Mirror Descent in Variationally Coherent Optimization Problems.
163,163,Yuanyuan Liu,The Chinese University of Hong Kong,Licheng Jiao,Xidian University,Accelerated First-order Methods for Geodesically Convex Optimization on Riemannian Manifolds.
164,164,Arturs Backurs,MIT,Ludwig Schmidt,MIT,On the Fine-Grained Complexity of Empirical Risk Minimization: Kernel Methods and Neural Networks.
165,165,Kinjal Basu,LinkedIn,Shaunak Chatterjee,LinkedIn,Large-Scale Quadratically Constrained Quadratic Program via Low-Discrepancy Sequences.
166,166,Sinong Wang,The Ohio State University,Ness Shroff,The Ohio State University,A New Alternating Direction Method for Linear Programming.
167,167,Ryan Tibshirani,Carnegie Mellon University,Ryan Tibshirani,Carnegie Mellon University,"Dykstra's Algorithm, ADMM, and Coordinate Descent: Connections, Insights, and Extensions."
168,168,Ahmet Alacaoglu,EPFL,Volkan Cevher,EPFL,Smooth Primal-Dual Coordinate Descent Algorithms for Nonsmooth Convex Optimization.
169,169,Aryan Mokhtari,University of Pennsylvania,Alejandro Ribeiro,University of Pennsylvania,First-Order Adaptive Sample Size Methods to Reduce Complexity of Empirical Risk Minimization.
170,170,Patrick Rebeschini,University of Oxford,Sekhar C Tatikonda,Yale University,Accelerated consensus via Min-Sum Splitting.
171,171,Damien Scieur,INRIA - ENS,Alexandre d'Aspremont,CNRS - Ecole Normale Supérieure,Integration Methods and Optimization Algorithms.
172,172,Celestine Dünner,IBM Research,Martin Jaggi,EPFL,Efficient Use of Limited-Memory Accelerators for Linear Learning on Heterogeneous Systems.
173,173,Charles Kuang,"University of Wisconsin, Madison",David Page,UW-Madison,A Screening Rule for l1-Regularized Ising Model Estimation.
174,174,Mark Rowland,University of Cambridge,Adrian Weller,University of Cambridge,Uprooting and Rerooting Higher-Order Graphical Models.
175,175,Constantinos Daskalakis,MIT,Gautam Kamath,MIT,Concentration of Multilinear Functions of the Ising Model with Applications to Network Data.
176,176,Murat Erdogdu,University of Toronto,Andrea Montanari,Stanford,Inference in Graphical Models via Semidefinite Programming Hierarchies.
177,177,Rebecca Morrison,Massachusetts Institute of Technology,Youssef Marzouk,Massachusetts Institute of Technology,Beyond normality: Learning sparse probabilistic graphical models in the non-Gaussian setting.
178,178,Qi Lou,UCI,Alexander Ihler,UC Irvine,Dynamic Importance Sampling for Anytime Bounds of the Partition Function.
